# 1. Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as snsS
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import KBinsDiscretizer
from google.colab import drive

# 2. Mount Google Drive
drive.mount('/content/drive')

# 3. Load dataset
file_path = "/content/drive/MyDrive/Tugas Akhir/TA 2025/DATA ALL dan SLOPE.csv"
df = pd.read_csv(file_path)

# 4. Aggregate accident count per segment per year
df_grouped = df.groupby(['ID', 'Year of Accident']).size().reset_index(name='Accident_Count')

# 5. Merge with road features
df_features = df.drop(columns=['Year of Accident'])
df_merged = df_grouped.merge(df_features.drop_duplicates(subset=['ID']), on='ID', how='left')

# 6. Split train and test sets
train_data = df_merged[df_merged["Year of Accident"] < 2024]
test_data = df_merged[df_merged["Year of Accident"] == 2024]

# 7. Select features and target variable
features = ['Road Type', 'Road Length (m)', 'Road Condition',
            'Number of Lanes', 'Intersection Type', 'Buffer Distance from Intersections',
            'Driver Behaviour', 'Vehicle Issue', 'Obstacle', 'Rush Hour','slope', 'Road Width (m)']
X_train = train_data[features]
y_train = train_data['Accident_Count']
X_test = test_data[features]

# 8. Stratified binning for K-Fold Validation
kbin = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
y_train_binned = kbin.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# 9. Hyperparameter tuning and K-Fold Cross Validation
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
model = RandomForestRegressor(random_state=42)
param_grid = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}
grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)

mae_scores, rmse_scores, r2_scores = [], [], []
all_preds, all_trues = [], []

print("=== K-Fold Cross Validation (10-Fold) ===")
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train_binned), 1):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    grid_search.fit(X_tr, y_tr)
    best_model = grid_search.best_estimator_
    y_pred_val = best_model.predict(X_val)

    mae = mean_absolute_error(y_val, y_pred_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
    r2 = r2_score(y_val, y_pred_val)

    mae_scores.append(mae)
    rmse_scores.append(rmse)
    r2_scores.append(r2)

    all_preds.extend(y_pred_val)
    all_trues.extend(y_val)

    print(f"Fold {fold} — MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}")

    mae_scores.append(mae)
    rmse_scores.append(rmse)
    r2_scores.append(r2)
    fold += 1

print("\n=== K-Fold Cross Validation Summary ===")
print(f"Average MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}")
print(f"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}")
print(f"Average R² Score: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}")

# 10. Train final model and predict test data
final_model = grid_search.best_estimator_
final_model.fit(X_train, y_train)
# Hasil fitting (prediksi pada data latih)
y_pred_train = final_model.predict(X_train)

# Visualisasi hasil fitting
plt.figure(figsize=(8, 6))
plt.scatter(y_train, y_pred_train, alpha=0.6, color='green', edgecolor='k')
plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], 'r--', label='Ideal Fit')
plt.xlabel("Actual Accident Count")
plt.ylabel("Predicted Accident Count")
plt.title("Hasil Fitting Model pada Data Latih")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
y_pred_test = final_model.predict(X_test)

# 11. Evaluate model performance on 2024 test data
if not test_data.empty:
    y_test = test_data['Accident_Count']
    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    r2 = r2_score(y_test, y_pred_test)

    print("\n=== Evaluasi pada Data Test (2024) ===")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²: {r2:.4f}")

# 11. Visualisasi hasil fitting (prediksi vs aktual)
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.regplot(x=all_trues, y=all_preds, scatter_kws={'alpha':0.5}, line_kws={"color": "red"})
plt.xlabel("Actual Accident Count")
plt.ylabel("Predicted Accident Count")
plt.title("Fitting Result on Validation Set (K-Fold)")
plt.grid(True)
plt.tight_layout()
plt.show()

# 11b. Visualisasi hasil fitting untuk data 2024
if 'Accident_Count' in test_data.columns:
    plt.figure(figsize=(8, 6))
    sns.regplot(x=test_data['Accident_Count'], y=y_pred_test, scatter_kws={'alpha':0.5}, line_kws={"color": "blue"})
    plt.xlabel("Actual Accident Count (2024)")
    plt.ylabel("Predicted Accident Count")
    plt.title("Prediction Result on Test Set (2024)")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# 12. Calculate feature importance
importances = final_model.feature_importances_
feat_importance = pd.DataFrame({'Feature': features, 'Importance': importances})
feat_importance = feat_importance.sort_values(by='Importance', ascending=False)
print("\n=== Feature Importance Ranking ===")
print(feat_importance)

# 14. Perform feature ablation study
ablation_results = []

for feature in X_train.columns:
    # Copy data training & delete one feature
    X_train_ablate = X_train.drop(columns=[feature])
    X_test_ablate = X_test.drop(columns=[feature])

    # Train new model
    model_ablate = RandomForestRegressor()
    model_ablate.fit(X_train_ablate, y_train)

    # Save ablation result
    ablation_results.append({
        'Removed_Feature': feature,
        'Model': model_ablate,
        'X_train_ablate': X_train_ablate  # Save to take the column
    })
target_feature = 'Road Length (m)'

# 15. Recalculate feature importance after ablation
for result in ablation_results:
    if result['Removed_Feature'] == target_feature:
        model_ablate = result['Model']
        remaining_features = result['X_train_ablate'].columns
        importances_ablate = model_ablate.feature_importances_

# 16. Visualize feature importance after ablation
feat_importance_ablate = pd.DataFrame({
    'Feature': remaining_features,
    'Importance': importances_ablate
}).sort_values(by='Importance', ascending=False)

print(f"\n=== Feature Importance Ranking Setelah Ablation (tanpa '{target_feature}') ===")
print(feat_importance_ablate)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_importance_ablate, palette='plasma')
plt.title(f'Feature Importance Setelah Ablation: Menghapus {target_feature}')
plt.tight_layout()
plt.show()
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
target_feature = 'Road Length (m)'

# 17. Evaluate test performance after ablation
for result in ablation_results:
    if result['Removed_Feature'] == target_feature:
        model_ablate = result['Model']
        X_test_ablate = X_test.drop(columns=[target_feature])

        # Prediksi dan evaluasi
        y_pred_ablate = model_ablate.predict(X_test_ablate)
        y_test = test_data['Accident_Count']

        mae_ablate = mean_absolute_error(y_test, y_pred_ablate)
        rmse_ablate = np.sqrt(mean_squared_error(y_test, y_pred_ablate))
        r2_ablate = r2_score(y_test, y_pred_ablate)

        print(f"\n=== Evaluasi Model Setelah Ablation ('{target_feature}') ===")
        print(f"MAE: {mae_ablate:.4f}")
        print(f"RMSE: {rmse_ablate:.4f}")
        print(f"R²: {r2_ablate:.4f}")
        break
print("\n=== K-Fold Cross Validation Setelah Ablation ===")
ablation_kfold_results = []

# 18. K-Fold evaluation after ablation
for ablate in ablation_results:
    removed_feature = ablate['Removed_Feature'] # Access 'Removed_Feature' key
    X_train_ablate = ablate['X_train_ablate']

    # Binning ulang karena X_train berubah
    y_train_binned_ablate = kbin.fit_transform(y_train.values.reshape(-1, 1)).ravel()

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    model = RandomForestRegressor(random_state=42)
    param_grid = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}
    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1)

    mae_scores, rmse_scores, r2_scores = [], [], []

    print(f"\nAblation: Menghapus fitur '{removed_feature}'") # Use removed_feature

    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_ablate, y_train_binned_ablate), 1):
        X_tr, X_val = X_train_ablate.iloc[train_idx], X_train_ablate.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        grid_search.fit(X_tr, y_tr)
        best_model = grid_search.best_estimator_
        y_pred_val = best_model.predict(X_val)

        mae = mean_absolute_error(y_val, y_pred_val)
        rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
        r2 = r2_score(y_val, y_pred_val)

        mae_scores.append(mae)
        rmse_scores.append(rmse)
        r2_scores.append(r2)

        print(f"Fold {fold} — MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}")

    print(f"Rata-rata (hapus '{removed_feature}'): MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}, " # Use removed_feature
          f"RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}, "
          f"R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}")

    ablation_kfold_results.append({
        'Removed Feature': removed_feature, # Use removed_feature
        'MAE_Mean': np.mean(mae_scores),
        'RMSE_Mean': np.mean(rmse_scores),
        'R2_Mean': np.mean(r2_scores)
    })

# 19. Save prediction
predictions_df = pd.DataFrame({ # Assign the DataFrame to predictions_df
    'ID': test_data['ID'].values,
    'Actual': test_data['Accident_Count'].values,
    'Predicted': y_pred_test
})
predictions_df.to_csv("/content/drive/MyDrive/Tugas Akhir/TA 2025/prediksi_2024.csv", index=False)

# Add the 'Predicted' column to test_data
test_data = test_data.merge(predictions_df[['ID', 'Predicted']], on='ID', how='left')
